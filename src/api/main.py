"""
AutoHDR Clone - FastAPI Backend
================================

REST API for real estate photo editing with full RAW file support.

Supports: ARW (Sony), CR2/CR3 (Canon), NEF (Nikon), DNG, RAF (Fuji), etc.

Run:
    ./start-backend.sh

Endpoints:
    POST /process   - Main processing (HDR merge or twilight)
    GET  /health    - Health check
    GET  /test      - Test processing with generated image
"""

import io
import time
import traceback
from pathlib import Path
from typing import List, Optional

import cv2
import numpy as np
from fastapi import FastAPI, File, UploadFile, HTTPException, Form, Query
from fastapi.responses import StreamingResponse, Response, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ============================================
# DEPENDENCY CHECKS
# ============================================

# RAW file support
try:
    import rawpy
    HAS_RAWPY = True
    RAWPY_VERSION = rawpy.__version__ if hasattr(rawpy, '__version__') else 'unknown'
except ImportError:
    HAS_RAWPY = False
    RAWPY_VERSION = None
    print("‚ö†Ô∏è  Warning: rawpy not installed. RAW file support disabled.")
    print("   Install with: pip install rawpy")

# OpenCV version
CV2_VERSION = cv2.__version__

# Import our processor
import sys
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

try:
    from src.core.processor import AutoHDRProcessor, ProcessingSettings, PROCESSOR_VERSION
    HAS_PROCESSOR = True
except ImportError as e:
    HAS_PROCESSOR = False
    PROCESSOR_VERSION = None
    print(f"‚ö†Ô∏è  Warning: Could not import processor: {e}")

# AI-enhanced processor (optional)
try:
    from src.core.processor_ai import AIEnhancedProcessor, AIProcessingSettings
    HAS_AI_PROCESSOR = True
except ImportError as e:
    HAS_AI_PROCESSOR = False
    print(f"‚ÑπÔ∏è  AI processor not available (install ai-requirements.txt for 90% quality)")

# Bulletproof Processor v6.0 - Production-grade, zero grain
try:
    from src.core.processor_bulletproof import BulletproofProcessor, BulletproofSettings, PROCESSOR_VERSION as BP_VERSION
    HAS_BULLETPROOF = True
    print(f"‚úì Bulletproof Processor v{BP_VERSION} loaded - Zero grain, crystal clear")
except ImportError as e:
    HAS_BULLETPROOF = False
    BP_VERSION = None
    print(f"‚ÑπÔ∏è  Bulletproof Processor not available: {e}")

# Turbo Mode - 4x faster processing (Apple Silicon optimized)
try:
    from src.core.performance import TurboProcessor, get_turbo_status, TURBO_VERSION
    HAS_TURBO = True
    turbo_status = get_turbo_status()
    print(f"üöÄ Turbo Mode v{TURBO_VERSION} - {turbo_status['opencv_threads']} threads, GPU: {turbo_status['gpu_available']}")
except ImportError as e:
    HAS_TURBO = False
    TURBO_VERSION = None

# Smart Processor v1.0 - Room-aware + Lens correction
try:
    from src.core.smart_processor import SmartProcessor, SMART_PROCESSOR_VERSION
    from src.core.room_classifier import RoomType
    HAS_SMART = True
    print(f"‚úì Smart Processor v{SMART_PROCESSOR_VERSION} loaded - Room detection + Lens correction")
except ImportError as e:
    HAS_SMART = False
    SMART_PROCESSOR_VERSION = None
    print(f"‚ÑπÔ∏è  Smart Processor not available: {e}")

# Clean Processor v5.0 - Fallback
try:
    from src.core.processor_clean import HDRitProcessor, Settings as CleanSettings, PROCESSOR_VERSION as CLEAN_VERSION
    HAS_CLEAN_PROCESSOR = True
except ImportError as e:
    HAS_CLEAN_PROCESSOR = False
    CLEAN_VERSION = None

# Legacy Pro Processor (fallback)
try:
    from src.core.processor_v3 import AutoHDRProProcessor, ProSettings, PROCESSOR_VERSION as PRO_VERSION
    HAS_PRO_PROCESSOR = True
except ImportError:
    HAS_PRO_PROCESSOR = False
    PRO_VERSION = None

# Optional modules
HAS_HDR_MERGER = False
HAS_TWILIGHT = False

# ============================================
# APP SETUP
# ============================================

app = FastAPI(
    title="HDRit API",
    description="Professional real estate photo editing with full RAW support",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
)

# CORS for web frontend - allow large file uploads
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "https://hdr.it.com",
        "https://autohdr-clone.vercel.app",
        "https://*.vercel.app",
        "*"  # Allow all for tunnel access
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["Content-Length", "X-Processing-Time-Ms", "X-Processor", "X-Images-Processed"],
    max_age=3600,  # Cache preflight for 1 hour
)


# ============================================
# STARTUP EVENT
# ============================================

@app.on_event("startup")
async def startup_event():
    """Print status on startup."""
    print("")
    print("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó")
    print("‚ïë           AutoHDR Clone - Backend Server                     ‚ïë")
    print("‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£")
    print(f"‚ïë  OpenCV:     {CV2_VERSION:<10} ‚úì                                 ‚ïë")
    if HAS_RAWPY:
        print(f"‚ïë  rawpy:      {RAWPY_VERSION:<10} ‚úì  (ARW/CR2/NEF support)       ‚ïë")
    else:
        print("‚ïë  rawpy:      NOT INSTALLED ‚úó  (RAW files disabled)        ‚ïë")
    if HAS_PROCESSOR:
        version_str = PROCESSOR_VERSION if PROCESSOR_VERSION else "Ready"
        print(f"‚ïë  Processor:  {version_str:<10} ‚úì                                   ‚ïë")
    else:
        print("‚ïë  Processor:  NOT FOUND  ‚úó                                   ‚ïë")
    print("‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£")
    print("‚ïë  Endpoints:                                                   ‚ïë")
    print("‚ïë    POST /process   - Process images (HDR or twilight)         ‚ïë")
    print("‚ïë    GET  /test      - Test processing pipeline                 ‚ïë")
    print("‚ïë    GET  /health    - Health check                             ‚ïë")
    print("‚ïë    GET  /docs      - API documentation                        ‚ïë")
    print("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù")
    print("")


# ============================================
# RAW FILE EXTENSIONS
# ============================================

RAW_EXTENSIONS = {
    '.arw', '.srf', '.sr2',  # Sony
    '.cr2', '.cr3', '.crw',  # Canon
    '.nef', '.nrw',          # Nikon
    '.dng',                  # Adobe
    '.orf',                  # Olympus
    '.rw2',                  # Panasonic
    '.pef', '.ptx',          # Pentax
    '.raf',                  # Fujifilm
    '.erf',                  # Epson
    '.mrw',                  # Minolta
    '.3fr', '.fff',          # Hasselblad
    '.iiq',                  # Phase One
    '.rwl',                  # Leica
    '.srw',                  # Samsung
    '.x3f',                  # Sigma
    '.raw',                  # Generic
}


# ============================================
# MODELS
# ============================================

class HDRMergeRequest(BaseModel):
    method: str = "mertens"
    tone_map: str = "reinhard"
    align: bool = True


class TwilightRequest(BaseModel):
    sky_intensity: float = 0.9
    window_glow: float = 0.8


class ProcessingResponse(BaseModel):
    success: bool
    job_id: str
    message: str


# ============================================
# HELPERS
# ============================================

def read_image_from_upload(file: UploadFile) -> np.ndarray:
    """Read uploaded file into OpenCV image, including RAW formats."""
    contents = file.file.read()
    filename = file.filename or "image.jpg"
    ext = Path(filename).suffix.lower()

    # Handle RAW files
    if ext in RAW_EXTENSIONS:
        if not HAS_RAWPY:
            raise HTTPException(400, f"RAW file support not available. Install rawpy.")
        try:
            with rawpy.imread(io.BytesIO(contents)) as raw:
                rgb = raw.postprocess(
                    use_camera_wb=True,
                    half_size=False,
                    no_auto_bright=False,
                    output_bps=8
                )
                # Convert RGB to BGR for OpenCV
                return cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)
        except Exception as e:
            raise HTTPException(400, f"Could not decode RAW file {filename}: {str(e)}")

    # Standard image formats
    nparr = np.frombuffer(contents, np.uint8)
    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    if image is None:
        raise HTTPException(400, f"Could not decode image: {filename}")
    return image


async def read_image_async(file: UploadFile) -> np.ndarray:
    """Async version of read_image_from_upload."""
    contents = await file.read()
    filename = file.filename or "image.jpg"
    ext = Path(filename).suffix.lower()

    # Handle RAW files
    if ext in RAW_EXTENSIONS:
        if not HAS_RAWPY:
            raise HTTPException(400, f"RAW file support not available. Install rawpy.")
        try:
            with rawpy.imread(io.BytesIO(contents)) as raw:
                rgb = raw.postprocess(
                    use_camera_wb=True,
                    half_size=False,
                    no_auto_bright=False,
                    output_bps=8
                )
                return cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)
        except Exception as e:
            raise HTTPException(400, f"Could not decode RAW file {filename}: {str(e)}")

    # Standard image formats
    nparr = np.frombuffer(contents, np.uint8)
    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    if image is None:
        raise HTTPException(400, f"Could not decode image: {filename}")
    return image


def image_to_bytes(image: np.ndarray, format: str = ".jpg", quality: int = 90) -> bytes:
    """Convert OpenCV image to bytes."""
    if format == ".jpg":
        encode_params = [cv2.IMWRITE_JPEG_QUALITY, quality]
    else:
        encode_params = []
    success, encoded = cv2.imencode(format, image, encode_params)
    if not success:
        raise HTTPException(500, "Failed to encode result image")
    return encoded.tobytes()


def merge_brackets_mertens(images: List[np.ndarray]) -> np.ndarray:
    """Merge bracketed exposures using Mertens exposure fusion."""
    if len(images) == 1:
        return images[0]

    # Ensure all images same size
    base_shape = images[0].shape[:2]
    resized = []
    for img in images:
        if img.shape[:2] != base_shape:
            img = cv2.resize(img, (base_shape[1], base_shape[0]))
        resized.append(img)

    # Mertens exposure fusion
    # IMPORTANT: Pass uint8 images directly - Mertens handles conversion internally
    merge_mertens = cv2.createMergeMertens()
    fusion = merge_mertens.process(resized)  # uint8 input, float32 output (0-1 range)
    fusion = np.clip(fusion * 255, 0, 255).astype(np.uint8)

    return fusion


def _build_gaussian_pyramid(img: np.ndarray, levels: int) -> List[np.ndarray]:
    """Build Gaussian pyramid."""
    pyramid = [img.astype(np.float32)]
    for _ in range(levels - 1):
        img = cv2.pyrDown(img)
        pyramid.append(img.astype(np.float32))
    return pyramid


def _build_laplacian_pyramid(img: np.ndarray, levels: int) -> List[np.ndarray]:
    """Build Laplacian pyramid from image."""
    gaussian = _build_gaussian_pyramid(img, levels)
    laplacian = []
    for i in range(levels - 1):
        size = (gaussian[i].shape[1], gaussian[i].shape[0])
        upsampled = cv2.pyrUp(gaussian[i + 1], dstsize=size)
        laplacian.append(gaussian[i] - upsampled)
    laplacian.append(gaussian[-1])  # Top level is just gaussian
    return laplacian


def _reconstruct_from_laplacian(pyramid: List[np.ndarray]) -> np.ndarray:
    """Reconstruct image from Laplacian pyramid."""
    img = pyramid[-1]
    for i in range(len(pyramid) - 2, -1, -1):
        size = (pyramid[i].shape[1], pyramid[i].shape[0])
        img = cv2.pyrUp(img, dstsize=size) + pyramid[i]
    return img


def _compute_weight_map(img: np.ndarray) -> np.ndarray:
    """
    Compute exposure fusion weight map based on:
    - Contrast (Laplacian magnitude)
    - Saturation
    - Well-exposedness (how close to middle gray)
    """
    img_float = img.astype(np.float32) / 255.0

    # Contrast weight (Laplacian filter response)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).astype(np.float32)
    laplacian = np.abs(cv2.Laplacian(gray, cv2.CV_32F))
    contrast = laplacian / (laplacian.max() + 1e-8)

    # Saturation weight
    saturation = img_float.std(axis=2)

    # Well-exposedness (Gaussian centered at 0.5)
    sigma = 0.2
    well_exposed = np.exp(-0.5 * ((img_float - 0.5) ** 2) / (sigma ** 2))
    well_exposed = np.prod(well_exposed, axis=2)  # Product across channels

    # Combined weight
    weight = (contrast ** 1.0) * (saturation ** 1.0) * (well_exposed ** 1.0)
    weight = weight + 1e-8  # Avoid division by zero

    return weight


def merge_brackets_laplacian(images: List[np.ndarray], levels: int = 6) -> np.ndarray:
    """
    Laplacian pyramid exposure fusion - better than basic Mertens.
    Based on "Exposure Fusion" by Mertens et al. with pyramid blending.
    """
    if len(images) == 1:
        return images[0]

    # Ensure all images same size
    base_shape = images[0].shape[:2]
    resized = []
    for img in images:
        if img.shape[:2] != base_shape:
            img = cv2.resize(img, (base_shape[1], base_shape[0]))
        resized.append(img)

    n_images = len(resized)

    # Compute weight maps for each image
    weights = [_compute_weight_map(img) for img in resized]

    # Normalize weights (sum to 1 at each pixel)
    weight_sum = np.sum(weights, axis=0) + 1e-8
    weights = [w / weight_sum for w in weights]

    # Build Gaussian pyramids of weights
    weight_pyramids = [_build_gaussian_pyramid(w, levels) for w in weights]

    # Build Laplacian pyramids of images
    laplacian_pyramids = [_build_laplacian_pyramid(img, levels) for img in resized]

    # Blend at each pyramid level
    blended_pyramid = []
    for level in range(levels):
        blended_level = np.zeros_like(laplacian_pyramids[0][level])
        for i in range(n_images):
            # Expand weight to 3 channels
            w = weight_pyramids[i][level]
            if len(blended_level.shape) == 3:
                w = np.expand_dims(w, axis=2)
            blended_level += w * laplacian_pyramids[i][level]
        blended_pyramid.append(blended_level)

    # Reconstruct from blended pyramid
    result = _reconstruct_from_laplacian(blended_pyramid)
    result = np.clip(result, 0, 255).astype(np.uint8)

    return result


def merge_brackets_middle_base(images: List[np.ndarray]) -> np.ndarray:
    """
    Professional real estate fusion: Middle exposure as base (neutral balance).

    Based on industry research:
    - Middle exposure provides balanced starting point (no blown highlights, no crushed shadows)
    - Blend BRIGHT image into: dark areas, shadows (for detail)
    - Blend DARK image into: light bulbs and glow areas (for fixture detail)

    No hard-edged spatial masks - only luminosity-based blending for natural results.
    """
    if len(images) == 1:
        return images[0]

    # Ensure all images same size
    base_shape = images[0].shape[:2]
    resized = []
    for img in images:
        if img.shape[:2] != base_shape:
            img = cv2.resize(img, (base_shape[1], base_shape[0]))
        resized.append(img)

    # Sort by average brightness
    brightness = [np.mean(img) for img in resized]
    sorted_indices = np.argsort(brightness)  # Ascending (darkest first)

    darkest = resized[sorted_indices[0]].astype(np.float32)
    brightest = resized[sorted_indices[-1]].astype(np.float32)

    # Middle exposure as base
    if len(resized) >= 3:
        middle_idx = sorted_indices[len(sorted_indices) // 2]
        middle = resized[middle_idx].astype(np.float32)
    else:
        # If only 2 images, blend them 50/50 as "middle"
        middle = (darkest + brightest) / 2

    print(f"   Fusion: Middle base (balanced), bright for shadows, dark for lights")

    # Start with middle exposure
    result = middle.copy()

    # Analyze middle image luminosity
    middle_gray = cv2.cvtColor(middle.astype(np.uint8), cv2.COLOR_BGR2GRAY).astype(np.float32)
    bright_gray = cv2.cvtColor(brightest.astype(np.uint8), cv2.COLOR_BGR2GRAY).astype(np.float32)

    # =========================================================
    # SHADOW/DARK AREA RECOVERY: Blend bright image aggressively
    # =========================================================
    # Where middle exposure is below midtone (<140), pull from bright image
    # This ensures corridor and right side get lifted to match left
    shadow_mask = np.clip((140 - middle_gray) / 100, 0, 1)  # Smooth ramp 40-140
    shadow_mask = cv2.GaussianBlur(shadow_mask, (51, 51), 0)
    shadow_mask = np.stack([shadow_mask] * 3, axis=-1)

    # Strong blend of bright image into darker areas (65%)
    result = result * (1 - shadow_mask * 0.65) + brightest * shadow_mask * 0.65

    # =========================================================
    # GLOBAL BRIGHTNESS: Heavy bright blend for even lighting
    # =========================================================
    # Push bright image hard to get uniform brightness across frame
    global_lift = 0.78  # 78% bright image - bright across entire image
    result = result * (1 - global_lift) + brightest * global_lift

    # =========================================================
    # LIGHT FIXTURE RECOVERY: Blend dark image into blown areas
    # =========================================================
    # Where middle exposure is very bright (>220), use dark image for detail
    highlight_mask = np.clip((middle_gray - 210) / 45, 0, 1)
    highlight_mask = cv2.GaussianBlur(highlight_mask, (31, 31), 0)
    highlight_mask = np.stack([highlight_mask] * 3, axis=-1)

    # Strong blend of dark image into highlights (70% for fixture detail)
    result = result * (1 - highlight_mask * 0.70) + darkest * highlight_mask * 0.70

    # =========================================================
    # EXTREME HIGHLIGHT RECOVERY: Light bulbs specifically
    # =========================================================
    # Where bright image is completely blown (>250), definitely use dark
    extreme_mask = np.clip((bright_gray - 245) / 10, 0, 1)
    extreme_mask = cv2.GaussianBlur(extreme_mask, (21, 21), 0)
    extreme_mask = np.stack([extreme_mask] * 3, axis=-1)

    # Very strong blend for actual light sources (85%)
    result = result * (1 - extreme_mask * 0.85) + darkest * extreme_mask * 0.85

    return np.clip(result, 0, 255).astype(np.uint8)


# ============================================
# ENHANCE MODE PROCESSING (Perfect Edit)
# ============================================

def apply_enhance_processing(
    img: np.ndarray,
    brightness: float = 0,
    contrast: float = 0,
    vibrance: float = 0,
    white_balance: float = 0,
    window_pull: bool = True,
    sky_enhance: bool = True,
    perspective_correct: bool = True,
    noise_reduction: bool = True,
    sharpening: bool = True,
) -> np.ndarray:
    """
    Apply Perfect Edit enhancement to a single image.
    This is the mode=enhance processing pipeline.
    """
    result = img.copy()
    h, w = result.shape[:2]

    # 1. NOISE REDUCTION
    if noise_reduction:
        result = cv2.fastNlMeansDenoisingColored(result, None, 6, 6, 7, 21)

    # 2. WHITE BALANCE CORRECTION
    if white_balance != 0:
        # Adjust color temperature
        lab = cv2.cvtColor(result, cv2.COLOR_BGR2LAB).astype(np.float32)
        lab[:, :, 2] = np.clip(lab[:, :, 2] + white_balance * 15, 0, 255)  # b channel (blue-yellow)
        result = cv2.cvtColor(lab.astype(np.uint8), cv2.COLOR_LAB2BGR)

    # 3. WINDOW PULL (balance window exposure)
    if window_pull:
        # Detect bright regions (windows) and balance them
        gray = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)
        _, bright_mask = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)
        bright_mask = cv2.GaussianBlur(bright_mask, (21, 21), 0)
        bright_mask_3ch = np.stack([bright_mask] * 3, axis=-1).astype(np.float32) / 255

        # Darken windows slightly to recover detail
        darkened = (result.astype(np.float32) * 0.75).astype(np.uint8)
        result = (result.astype(np.float32) * (1 - bright_mask_3ch * 0.4) +
                  darkened.astype(np.float32) * bright_mask_3ch * 0.4).astype(np.uint8)

    # 4. SKY ENHANCEMENT (boost blue sky)
    if sky_enhance:
        # Target upper portion of image and blue regions
        hsv = cv2.cvtColor(result, cv2.COLOR_BGR2HSV).astype(np.float32)
        # Blue hue range: 100-130
        blue_mask = ((hsv[:, :, 0] > 90) & (hsv[:, :, 0] < 135) &
                     (hsv[:, :, 1] > 30)).astype(np.float32)
        # Weight by vertical position (stronger at top)
        y_weight = np.linspace(1.0, 0.0, h).reshape(-1, 1)
        blue_mask = blue_mask * y_weight
        blue_mask = cv2.GaussianBlur(blue_mask.astype(np.float32), (31, 31), 0)

        # Boost saturation in sky areas
        hsv[:, :, 1] = np.clip(hsv[:, :, 1] + blue_mask * 40, 0, 255)
        hsv[:, :, 2] = np.clip(hsv[:, :, 2] + blue_mask * 10, 0, 255)
        result = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)

    # 5. PERSPECTIVE CORRECTION (straighten verticals)
    if perspective_correct and h > 100 and w > 100:
        # Simple vertical straightening using edge detection
        gray = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 50, 150, apertureSize=3)
        lines = cv2.HoughLinesP(edges, 1, np.pi/180, 100, minLineLength=h//4, maxLineGap=20)

        if lines is not None and len(lines) > 0:
            # Find near-vertical lines
            angles = []
            for line in lines:
                x1, y1, x2, y2 = line[0]
                if abs(x2 - x1) < abs(y2 - y1):  # More vertical than horizontal
                    angle = np.arctan2(x2 - x1, y2 - y1) * 180 / np.pi
                    if abs(angle) < 15:  # Within 15 degrees of vertical
                        angles.append(angle)

            if len(angles) > 2:
                # Compute median angle correction
                correction = np.median(angles)
                if abs(correction) > 0.5 and abs(correction) < 10:
                    # Apply rotation to straighten
                    center = (w // 2, h // 2)
                    M = cv2.getRotationMatrix2D(center, correction, 1.0)
                    result = cv2.warpAffine(result, M, (w, h), borderMode=cv2.BORDER_REFLECT)

    # 6. BRIGHTNESS & CONTRAST
    if brightness != 0 or contrast != 0:
        result = result.astype(np.float32)
        # Brightness: shift
        result = result + brightness * 30
        # Contrast: scale around mid
        if contrast != 0:
            factor = 1 + contrast * 0.3
            result = (result - 128) * factor + 128
        result = np.clip(result, 0, 255).astype(np.uint8)

    # 7. VIBRANCE (smart saturation)
    if vibrance != 0:
        hsv = cv2.cvtColor(result, cv2.COLOR_BGR2HSV).astype(np.float32)
        # Only boost low-saturation areas (vibrance vs saturation)
        sat = hsv[:, :, 1]
        boost = (1 - sat / 255) * vibrance * 30  # Less saturated = more boost
        hsv[:, :, 1] = np.clip(sat + boost, 0, 255)
        result = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)

    # 8. SHARPENING
    if sharpening:
        # Unsharp mask
        blurred = cv2.GaussianBlur(result, (0, 0), 2.0)
        result = cv2.addWeighted(result, 1.3, blurred, -0.3, 0)

    return result


# ============================================
# ROUTES
# ============================================

@app.get("/")
async def root():
    """API info and capabilities."""
    return {
        "name": "AutoHDR Clone API",
        "version": "1.0.0",
        "status": "running",
        "capabilities": {
            "raw_support": HAS_RAWPY,
            "processor": HAS_PROCESSOR,
            "opencv": CV2_VERSION,
        },
        "supported_formats": {
            "standard": ["jpg", "jpeg", "png", "tiff", "bmp", "webp"],
            "raw": list(RAW_EXTENSIONS) if HAS_RAWPY else []
        },
        "endpoints": {
            "POST /process": "Main processing endpoint",
            "GET /health": "Health check",
            "GET /test": "Test processing pipeline",
            "GET /docs": "API documentation"
        }
    }


@app.get("/health")
async def health():
    """Health check - returns system status."""
    return {
        "status": "healthy",
        "components": {
            "rawpy": {"installed": HAS_RAWPY, "version": RAWPY_VERSION},
            "opencv": {"installed": True, "version": CV2_VERSION},
            "processor": {
                "installed": HAS_PROCESSOR,
                "version": PROCESSOR_VERSION,
                "features": [
                    "CLAHE tone mapping",
                    "Laplacian pyramid fusion",
                    "Cool white balance correction",
                    "Shadow recovery",
                    "S-curve contrast",
                    "Non-local means denoising",
                    "Bilateral edge smoothing",
                    "Local contrast (clarity)",
                    "LAB + HSV color boost",
                ] if HAS_PROCESSOR else []
            },
            "ai_processor": {"installed": HAS_AI_PROCESSOR, "note": "SAM + YOLOv8 + LaMa"},
            "turbo_mode": get_turbo_status() if HAS_TURBO else {"turbo_available": False},
            "pro_processor": {
                "installed": HAS_PRO_PROCESSOR,
                "version": PRO_VERSION if HAS_PRO_PROCESSOR else None,
                "features": [
                    "Mertens Exposure Fusion",
                    "ECC Bracket Alignment",
                    "Window Detail Recovery",
                    "Flambient Tone Mapping",
                    "Professional Window Pull",
                    "Edge-Aware Processing",
                ] if HAS_PRO_PROCESSOR else []
            },
        },
        "raw_formats_supported": len(RAW_EXTENSIONS) if HAS_RAWPY else 0,
        "pro_processor_available": HAS_PRO_PROCESSOR,
        "quality_level": "95%+ AutoHDR" if HAS_PRO_PROCESSOR else ("90% AutoHDR" if HAS_AI_PROCESSOR else "60% AutoHDR"),
    }


@app.get("/test")
async def test_processing():
    """
    Test the processing pipeline with a generated gradient image.
    Returns a processed test image to verify the pipeline works.
    """
    if not HAS_PROCESSOR:
        raise HTTPException(500, "Processor not available")

    try:
        start_time = time.time()

        # Create a test gradient image (simulates a photo with shadows and highlights)
        height, width = 480, 640
        test_image = np.zeros((height, width, 3), dtype=np.uint8)

        # Gradient background (sky simulation)
        for y in range(height):
            ratio = y / height
            test_image[y, :] = [
                int(255 * (1 - ratio * 0.5)),  # Blue (sky to ground)
                int(200 * (1 - ratio * 0.3)),  # Green
                int(180 * (1 - ratio * 0.4)),  # Red
            ]

        # Add some "windows" (bright rectangles)
        cv2.rectangle(test_image, (100, 200), (200, 350), (255, 255, 255), -1)
        cv2.rectangle(test_image, (450, 180), (550, 320), (255, 255, 255), -1)

        # Add "shadows" (dark areas)
        cv2.rectangle(test_image, (250, 350), (400, 450), (30, 30, 30), -1)

        # Process with default settings
        processor = AutoHDRProcessor(ProcessingSettings())
        result = processor.process(test_image)

        elapsed_ms = (time.time() - start_time) * 1000

        # Encode as JPEG
        _, buffer = cv2.imencode('.jpg', result, [cv2.IMWRITE_JPEG_QUALITY, 85])

        return Response(
            content=buffer.tobytes(),
            media_type="image/jpeg",
            headers={
                "Content-Disposition": "inline; filename=test_result.jpg",
                "X-Processing-Time-Ms": str(round(elapsed_ms, 2)),
            }
        )

    except Exception as e:
        raise HTTPException(500, f"Test failed: {str(e)}")


# ============================================
# MAIN PROCESSING ENDPOINT (matches Vercel frontend)
# ============================================

@app.post("/process")
async def process_images(
    images: List[UploadFile] = File(..., description="Images to process"),
    mode: str = Query("hdr", description="Processing mode: hdr, twilight, or enhance"),
    brightness: float = Query(0, ge=-2, le=2),
    contrast: float = Query(0, ge=-2, le=2),
    vibrance: float = Query(0, ge=-2, le=2),
    whiteBalance: float = Query(0, ge=-2, le=2),
    white_balance: float = Query(None, ge=-2, le=2, description="Alias for whiteBalance"),
    # Perfect Edit mode parameters (mode=enhance)
    window_pull: bool = Query(True, description="Balance window exposure (enhance mode)"),
    sky_enhance: bool = Query(True, description="Boost blue sky (enhance mode)"),
    perspective_correct: bool = Query(True, description="Straighten verticals (enhance mode)"),
    noise_reduction: bool = Query(True, description="Reduce grain (enhance mode)"),
    sharpening: bool = Query(True, description="Sharpen details (enhance mode)"),
    # Legacy params
    ai: bool = Query(False, description="Use AI-enhanced processing (SAM, YOLOv8, LaMa)"),
    grass: bool = Query(False, description="Enhance grass (vibrant green)"),
    signs: bool = Query(False, description="Remove signs (requires ai=true for best results)"),
):
    """
    Main processing endpoint - matches Vercel frontend API.

    Modes:
    - **hdr**: 1 image = single-exposure HDR enhancement, 2+ = bracket merge
    - **twilight**: 1 image = day-to-dusk conversion
    - **enhance**: 1 image = full AI enhancement with Perfect Edit options

    Supports all RAW formats: ARW, CR2, NEF, DNG, etc.
    """
    start_time = time.time()

    # Support both whiteBalance and white_balance params
    wb = white_balance if white_balance is not None else whiteBalance

    # Validate
    if not images:
        raise HTTPException(400, "No images provided")

    if not HAS_PROCESSOR:
        raise HTTPException(500, "Processor not available - check server logs")

    # Validate mode
    if mode not in ["hdr", "twilight", "enhance"]:
        raise HTTPException(400, f"Invalid mode: {mode}. Use 'hdr', 'twilight', or 'enhance'")

    # Log request
    filenames = [img.filename for img in images]
    print(f"üì∏ Processing {len(images)} images: {filenames}")
    print(f"   Mode: {mode}, Settings: b={brightness}, c={contrast}, v={vibrance}, wb={wb}")
    if mode == "enhance":
        print(f"   Perfect Edit: window={window_pull}, sky={sky_enhance}, persp={perspective_correct}, denoise={noise_reduction}, sharp={sharpening}")

    try:
        # ==========================================
        # STEP 1: Read all images (including RAW)
        # ==========================================
        image_arrays = []
        for i, upload in enumerate(images):
            print(f"   Reading [{i+1}/{len(images)}]: {upload.filename}")
            try:
                img = await read_image_async(upload)
                print(f"   ‚úì Loaded: {img.shape[1]}x{img.shape[0]} pixels")
                image_arrays.append(img)
            except HTTPException:
                raise
            except Exception as e:
                raise HTTPException(400, f"Failed to read {upload.filename}: {str(e)}")

        # ==========================================
        # STEP 2: Mode-specific processing
        # ==========================================

        # ENHANCE MODE: Full AI enhancement with Perfect Edit options
        if mode == "enhance":
            print(f"   üé® Perfect Edit mode: {len(image_arrays)} image(s)")
            results = []
            for idx, img in enumerate(image_arrays):
                enhanced = apply_enhance_processing(
                    img,
                    brightness=brightness,
                    contrast=contrast,
                    vibrance=vibrance,
                    white_balance=wb,
                    window_pull=window_pull,
                    sky_enhance=sky_enhance,
                    perspective_correct=perspective_correct,
                    noise_reduction=noise_reduction,
                    sharpening=sharpening,
                )
                results.append(enhanced)
                print(f"   ‚úì Enhanced image {idx+1}/{len(image_arrays)}")

            # Return single image or batch
            if len(results) == 1:
                result_bytes = image_to_bytes(results[0], ".jpg", quality=95)
                elapsed_ms = (time.time() - start_time) * 1000
                return Response(
                    content=result_bytes,
                    media_type="image/jpeg",
                    headers={
                        "Content-Disposition": f'attachment; filename="hdrit_enhance.jpg"',
                        "Content-Length": str(len(result_bytes)),
                        "X-Processing-Time-Ms": str(round(elapsed_ms, 2)),
                        "X-Images-Processed": "1",
                        "X-Processor": f"Pro v{PRO_VERSION}" if HAS_PRO_PROCESSOR else "Standard",
                        "Access-Control-Allow-Origin": "*",
                    }
                )
            else:
                # Multiple images - return as ZIP
                import zipfile
                zip_buffer = io.BytesIO()
                with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zf:
                    for i, result in enumerate(results):
                        img_bytes = image_to_bytes(result, ".jpg", quality=95)
                        zf.writestr(f"hdrit_enhance_{i+1:02d}.jpg", img_bytes)
                zip_buffer.seek(0)
                elapsed_ms = (time.time() - start_time) * 1000
                return Response(
                    content=zip_buffer.getvalue(),
                    media_type="application/zip",
                    headers={
                        "Content-Disposition": f'attachment; filename="hdrit_enhance_batch.zip"',
                        "X-Processing-Time-Ms": str(round(elapsed_ms, 2)),
                        "X-Images-Processed": str(len(results)),
                    }
                )

        # HDR MODE: Single image enhancement or bracket merge
        # Use Bulletproof Processor v6.0 (production-grade, zero grain)
        if len(image_arrays) > 1 and HAS_BULLETPROOF:
            # Use SAME settings as single image (tuned to 99% AutoHDR match)
            bp_settings = BulletproofSettings(
                preset='professional',
                denoise_strength='heavy',
                sharpen=False,
                brighten=True,
                brighten_amount=1.5,  # Tuned to match target
            )
            processor = BulletproofProcessor(bp_settings)

            # Wrap with TurboProcessor for 4x speed boost
            if HAS_TURBO:
                processor = TurboProcessor(processor)
                print(f"   üöÄ Using Bulletproof v{BP_VERSION} + Turbo Mode for {len(image_arrays)} brackets...")
            else:
                print(f"   üî• Using Bulletproof Processor v{BP_VERSION} for {len(image_arrays)} brackets...")

            result = processor.process_brackets(image_arrays)

            # Encode and return
            result_bytes = image_to_bytes(result, ".jpg", quality=95)
            elapsed_ms = (time.time() - start_time) * 1000
            print(f"   ‚úì Clean Processor complete in {elapsed_ms:.0f}ms")
            print(f"   üì¶ Response size: {len(result_bytes) / 1024:.1f} KB")

            return Response(
                content=result_bytes,
                media_type="image/jpeg",
                headers={
                    "Content-Disposition": f'attachment; filename="hdrit_{mode}.jpg"',
                    "Content-Length": str(len(result_bytes)),
                    "X-Processing-Time-Ms": str(round(elapsed_ms, 2)),
                    "X-Images-Processed": str(len(images)),
                    "X-Processor": f"Clean v{CLEAN_VERSION}",
                    "Access-Control-Allow-Origin": "*",
                    "Access-Control-Expose-Headers": "Content-Length, X-Processing-Time-Ms, X-Processor",
                    "Cache-Control": "no-cache",
                }
            )

        # Single image processing with bulletproof processor
        base_image = image_arrays[0]
        print("   Applying HDR processing with Bulletproof Processor...")

        if HAS_BULLETPROOF:
            bp_settings = BulletproofSettings(
                preset='professional',
                denoise_strength='heavy',  # Heavy denoise (preserves detail)
                sharpen=False,  # AutoHDR is SOFT - no sharpening
                brighten=True,
                brighten_amount=1.5,  # Tuned to match target
            )
            processor = BulletproofProcessor(bp_settings)
            # Wrap with TurboProcessor for 4x speed boost
            if HAS_TURBO:
                processor = TurboProcessor(processor)
                proc_version = f"Bulletproof v{BP_VERSION} + Turbo"
            else:
                proc_version = f"Bulletproof v{BP_VERSION}"
            result = processor.process(base_image)
        elif HAS_CLEAN_PROCESSOR:
            clean_settings = CleanSettings(preset='natural')
            processor = HDRitProcessor(clean_settings)
            result = processor.process(base_image)
            proc_version = f"Clean v{CLEAN_VERSION}"
        else:
            # Legacy fallback
            settings = ProcessingSettings(
                brightness=brightness,
                contrast=contrast,
                vibrance=vibrance,
                white_balance=whiteBalance,
            )
            processor = AutoHDRProcessor(settings)
            result = processor.process(base_image)
            proc_version = "Legacy"

        # Encode and return
        result_bytes = image_to_bytes(result, ".jpg", quality=95)

        elapsed_ms = (time.time() - start_time) * 1000
        print(f"   ‚úì Complete in {elapsed_ms:.0f}ms, output: {len(result_bytes)} bytes")

        return Response(
            content=result_bytes,
            media_type="image/jpeg",
            headers={
                "Content-Disposition": f'attachment; filename="hdrit_{mode}.jpg"',
                "X-Processing-Time-Ms": str(round(elapsed_ms, 2)),
                "X-Images-Processed": str(len(images)),
                "X-Processor": proc_version,
            }
        )

    except HTTPException:
        raise
    except Exception as e:
        print(f"   ‚úó ERROR: {str(e)}")
        traceback.print_exc()
        raise HTTPException(500, f"Processing failed: {str(e)}")


# ============================================
# LEGACY ENDPOINTS (for direct API use)
# ============================================

@app.post("/hdr/merge")
async def hdr_merge(
    images: List[UploadFile] = File(...),
    method: str = Form("mertens"),
    tone_map: str = Form("reinhard"),
    align: bool = Form(True)
):
    """
    Merge multiple exposure brackets into HDR.

    Upload 2-9 images taken at different exposures.
    Returns the merged, tone-mapped result.
    """
    if len(images) < 2:
        raise HTTPException(400, "Need at least 2 images for HDR merge")
    if len(images) > 9:
        raise HTTPException(400, "Maximum 9 images supported")

    # Read all images (with RAW support)
    loaded_images = []
    for img_file in images:
        img = await read_image_async(img_file)
        loaded_images.append(img)

    # Use Mertens fusion (built-in, always works)
    result = merge_brackets_mertens(loaded_images)

    # Apply HDR processing
    processor = AutoHDRProcessor(ProcessingSettings())
    result = processor.process(result)

    # Return as JPEG
    result_bytes = image_to_bytes(result, ".jpg", quality=90)

    return StreamingResponse(
        io.BytesIO(result_bytes),
        media_type="image/jpeg",
        headers={"Content-Disposition": "attachment; filename=hdr_result.jpg"}
    )


@app.post("/effects/day-to-dusk")
async def day_to_dusk(
    image: UploadFile = File(...),
    style: str = Form("pink"),  # pink, blue, orange
    brightness: float = Form(-0.5),  # -2 to 2
):
    """
    Convert daytime exterior photo to twilight/dusk.

    Parameters:
    - style: Twilight color (pink, blue, orange)
    - brightness: Adjustment (-2 to 2, negative = darker)
    """
    # Read image (with RAW support)
    img = await read_image_async(image)

    # Configure for twilight
    settings = ProcessingSettings(
        brightness=brightness,
        twilight_style=style if style in ["pink", "blue", "orange"] else "pink"
    )

    # Process
    processor = AutoHDRProcessor(settings)
    result = processor.process(img)

    # Return as JPEG
    result_bytes = image_to_bytes(result, ".jpg")

    return StreamingResponse(
        io.BytesIO(result_bytes),
        media_type="image/jpeg",
        headers={"Content-Disposition": "attachment; filename=twilight_result.jpg"}
    )


@app.post("/edit/remove")
async def remove_object(
    image: UploadFile = File(...),
    prompt: str = Form(None),
    mask: UploadFile = File(None)
):
    """
    Remove objects from image using AI inpainting.

    Provide either:
    - prompt: Text description of what to remove (e.g., "remove the car")
    - mask: Binary mask image where white = area to remove

    Coming soon: Requires LaMa or IOPaint integration.
    """
    # TODO: Implement with LaMa / Grounded-SAM
    raise HTTPException(501, "Object removal coming soon. Use IOPaint for now: https://github.com/Sanster/IOPaint")


@app.post("/edit/sky-replace")
async def sky_replace(
    image: UploadFile = File(...),
    sky_type: str = Form("twilight")  # twilight, blue, dramatic, cloudy
):
    """
    Replace sky in image.

    Coming soon: Requires sky segmentation model.
    """
    # TODO: Implement with sky segmentation
    raise HTTPException(501, "Sky replacement coming soon. Requires segmentation model.")


# ============================================
# MAIN
# ============================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
