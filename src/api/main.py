"""
AutoHDR Clone - FastAPI Backend
================================

REST API for real estate photo editing with full RAW file support.

Supports: ARW (Sony), CR2/CR3 (Canon), NEF (Nikon), DNG, RAF (Fuji), etc.

Run:
    ./start-backend.sh

Endpoints:
    POST /process   - Main processing (HDR merge or twilight)
    GET  /health    - Health check
    GET  /test      - Test processing with generated image
"""

import io
import time
import traceback
from pathlib import Path
from typing import List, Optional

import cv2
import numpy as np
from fastapi import FastAPI, File, UploadFile, HTTPException, Form, Query
from fastapi.responses import StreamingResponse, Response, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ============================================
# DEPENDENCY CHECKS
# ============================================

# RAW file support
try:
    import rawpy
    HAS_RAWPY = True
    RAWPY_VERSION = rawpy.__version__ if hasattr(rawpy, '__version__') else 'unknown'
except ImportError:
    HAS_RAWPY = False
    RAWPY_VERSION = None
    print("‚ö†Ô∏è  Warning: rawpy not installed. RAW file support disabled.")
    print("   Install with: pip install rawpy")

# OpenCV version
CV2_VERSION = cv2.__version__

# Import our processor
import sys
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

try:
    from src.core.processor import AutoHDRProcessor, ProcessingSettings
    HAS_PROCESSOR = True
except ImportError as e:
    HAS_PROCESSOR = False
    print(f"‚ö†Ô∏è  Warning: Could not import processor: {e}")

# AI-enhanced processor (optional)
try:
    from src.core.processor_ai import AIEnhancedProcessor, AIProcessingSettings
    HAS_AI_PROCESSOR = True
except ImportError as e:
    HAS_AI_PROCESSOR = False
    print(f"‚ÑπÔ∏è  AI processor not available (install ai-requirements.txt for 90% quality)")

# Optional modules
HAS_HDR_MERGER = False
HAS_TWILIGHT = False

# ============================================
# APP SETUP
# ============================================

app = FastAPI(
    title="AutoHDR Clone API",
    description="Open-source AI real estate photo editing with RAW support",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
)

# CORS for web frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "https://autohdr-clone.vercel.app",
        "https://*.vercel.app",
        "*"  # Allow all for development
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============================================
# STARTUP EVENT
# ============================================

@app.on_event("startup")
async def startup_event():
    """Print status on startup."""
    print("")
    print("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó")
    print("‚ïë           AutoHDR Clone - Backend Server                     ‚ïë")
    print("‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£")
    print(f"‚ïë  OpenCV:     {CV2_VERSION:<10} ‚úì                                 ‚ïë")
    if HAS_RAWPY:
        print(f"‚ïë  rawpy:      {RAWPY_VERSION:<10} ‚úì  (ARW/CR2/NEF support)       ‚ïë")
    else:
        print("‚ïë  rawpy:      NOT INSTALLED ‚úó  (RAW files disabled)        ‚ïë")
    if HAS_PROCESSOR:
        print("‚ïë  Processor:  Ready      ‚úì                                   ‚ïë")
    else:
        print("‚ïë  Processor:  NOT FOUND  ‚úó                                   ‚ïë")
    print("‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£")
    print("‚ïë  Endpoints:                                                   ‚ïë")
    print("‚ïë    POST /process   - Process images (HDR or twilight)         ‚ïë")
    print("‚ïë    GET  /test      - Test processing pipeline                 ‚ïë")
    print("‚ïë    GET  /health    - Health check                             ‚ïë")
    print("‚ïë    GET  /docs      - API documentation                        ‚ïë")
    print("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù")
    print("")


# ============================================
# RAW FILE EXTENSIONS
# ============================================

RAW_EXTENSIONS = {
    '.arw', '.srf', '.sr2',  # Sony
    '.cr2', '.cr3', '.crw',  # Canon
    '.nef', '.nrw',          # Nikon
    '.dng',                  # Adobe
    '.orf',                  # Olympus
    '.rw2',                  # Panasonic
    '.pef', '.ptx',          # Pentax
    '.raf',                  # Fujifilm
    '.erf',                  # Epson
    '.mrw',                  # Minolta
    '.3fr', '.fff',          # Hasselblad
    '.iiq',                  # Phase One
    '.rwl',                  # Leica
    '.srw',                  # Samsung
    '.x3f',                  # Sigma
    '.raw',                  # Generic
}


# ============================================
# MODELS
# ============================================

class HDRMergeRequest(BaseModel):
    method: str = "mertens"
    tone_map: str = "reinhard"
    align: bool = True


class TwilightRequest(BaseModel):
    sky_intensity: float = 0.9
    window_glow: float = 0.8


class ProcessingResponse(BaseModel):
    success: bool
    job_id: str
    message: str


# ============================================
# HELPERS
# ============================================

def read_image_from_upload(file: UploadFile) -> np.ndarray:
    """Read uploaded file into OpenCV image, including RAW formats."""
    contents = file.file.read()
    filename = file.filename or "image.jpg"
    ext = Path(filename).suffix.lower()

    # Handle RAW files
    if ext in RAW_EXTENSIONS:
        if not HAS_RAWPY:
            raise HTTPException(400, f"RAW file support not available. Install rawpy.")
        try:
            with rawpy.imread(io.BytesIO(contents)) as raw:
                rgb = raw.postprocess(
                    use_camera_wb=True,
                    half_size=False,
                    no_auto_bright=False,
                    output_bps=8
                )
                # Convert RGB to BGR for OpenCV
                return cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)
        except Exception as e:
            raise HTTPException(400, f"Could not decode RAW file {filename}: {str(e)}")

    # Standard image formats
    nparr = np.frombuffer(contents, np.uint8)
    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    if image is None:
        raise HTTPException(400, f"Could not decode image: {filename}")
    return image


async def read_image_async(file: UploadFile) -> np.ndarray:
    """Async version of read_image_from_upload."""
    contents = await file.read()
    filename = file.filename or "image.jpg"
    ext = Path(filename).suffix.lower()

    # Handle RAW files
    if ext in RAW_EXTENSIONS:
        if not HAS_RAWPY:
            raise HTTPException(400, f"RAW file support not available. Install rawpy.")
        try:
            with rawpy.imread(io.BytesIO(contents)) as raw:
                rgb = raw.postprocess(
                    use_camera_wb=True,
                    half_size=False,
                    no_auto_bright=False,
                    output_bps=8
                )
                return cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)
        except Exception as e:
            raise HTTPException(400, f"Could not decode RAW file {filename}: {str(e)}")

    # Standard image formats
    nparr = np.frombuffer(contents, np.uint8)
    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    if image is None:
        raise HTTPException(400, f"Could not decode image: {filename}")
    return image


def image_to_bytes(image: np.ndarray, format: str = ".jpg", quality: int = 90) -> bytes:
    """Convert OpenCV image to bytes."""
    if format == ".jpg":
        encode_params = [cv2.IMWRITE_JPEG_QUALITY, quality]
    else:
        encode_params = []
    success, encoded = cv2.imencode(format, image, encode_params)
    if not success:
        raise HTTPException(500, "Failed to encode result image")
    return encoded.tobytes()


def merge_brackets_mertens(images: List[np.ndarray]) -> np.ndarray:
    """Merge bracketed exposures using Mertens exposure fusion."""
    if len(images) == 1:
        return images[0]

    # Ensure all images same size
    base_shape = images[0].shape[:2]
    resized = []
    for img in images:
        if img.shape[:2] != base_shape:
            img = cv2.resize(img, (base_shape[1], base_shape[0]))
        resized.append(img)

    # Mertens exposure fusion
    # IMPORTANT: Pass uint8 images directly - Mertens handles conversion internally
    merge_mertens = cv2.createMergeMertens()
    fusion = merge_mertens.process(resized)  # uint8 input, float32 output (0-1 range)
    fusion = np.clip(fusion * 255, 0, 255).astype(np.uint8)

    return fusion


def _build_gaussian_pyramid(img: np.ndarray, levels: int) -> List[np.ndarray]:
    """Build Gaussian pyramid."""
    pyramid = [img.astype(np.float32)]
    for _ in range(levels - 1):
        img = cv2.pyrDown(img)
        pyramid.append(img.astype(np.float32))
    return pyramid


def _build_laplacian_pyramid(img: np.ndarray, levels: int) -> List[np.ndarray]:
    """Build Laplacian pyramid from image."""
    gaussian = _build_gaussian_pyramid(img, levels)
    laplacian = []
    for i in range(levels - 1):
        size = (gaussian[i].shape[1], gaussian[i].shape[0])
        upsampled = cv2.pyrUp(gaussian[i + 1], dstsize=size)
        laplacian.append(gaussian[i] - upsampled)
    laplacian.append(gaussian[-1])  # Top level is just gaussian
    return laplacian


def _reconstruct_from_laplacian(pyramid: List[np.ndarray]) -> np.ndarray:
    """Reconstruct image from Laplacian pyramid."""
    img = pyramid[-1]
    for i in range(len(pyramid) - 2, -1, -1):
        size = (pyramid[i].shape[1], pyramid[i].shape[0])
        img = cv2.pyrUp(img, dstsize=size) + pyramid[i]
    return img


def _compute_weight_map(img: np.ndarray) -> np.ndarray:
    """
    Compute exposure fusion weight map based on:
    - Contrast (Laplacian magnitude)
    - Saturation
    - Well-exposedness (how close to middle gray)
    """
    img_float = img.astype(np.float32) / 255.0

    # Contrast weight (Laplacian filter response)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).astype(np.float32)
    laplacian = np.abs(cv2.Laplacian(gray, cv2.CV_32F))
    contrast = laplacian / (laplacian.max() + 1e-8)

    # Saturation weight
    saturation = img_float.std(axis=2)

    # Well-exposedness (Gaussian centered at 0.5)
    sigma = 0.2
    well_exposed = np.exp(-0.5 * ((img_float - 0.5) ** 2) / (sigma ** 2))
    well_exposed = np.prod(well_exposed, axis=2)  # Product across channels

    # Combined weight
    weight = (contrast ** 1.0) * (saturation ** 1.0) * (well_exposed ** 1.0)
    weight = weight + 1e-8  # Avoid division by zero

    return weight


def merge_brackets_laplacian(images: List[np.ndarray], levels: int = 6) -> np.ndarray:
    """
    Laplacian pyramid exposure fusion - better than basic Mertens.
    Based on "Exposure Fusion" by Mertens et al. with pyramid blending.
    """
    if len(images) == 1:
        return images[0]

    # Ensure all images same size
    base_shape = images[0].shape[:2]
    resized = []
    for img in images:
        if img.shape[:2] != base_shape:
            img = cv2.resize(img, (base_shape[1], base_shape[0]))
        resized.append(img)

    n_images = len(resized)

    # Compute weight maps for each image
    weights = [_compute_weight_map(img) for img in resized]

    # Normalize weights (sum to 1 at each pixel)
    weight_sum = np.sum(weights, axis=0) + 1e-8
    weights = [w / weight_sum for w in weights]

    # Build Gaussian pyramids of weights
    weight_pyramids = [_build_gaussian_pyramid(w, levels) for w in weights]

    # Build Laplacian pyramids of images
    laplacian_pyramids = [_build_laplacian_pyramid(img, levels) for img in resized]

    # Blend at each pyramid level
    blended_pyramid = []
    for level in range(levels):
        blended_level = np.zeros_like(laplacian_pyramids[0][level])
        for i in range(n_images):
            # Expand weight to 3 channels
            w = weight_pyramids[i][level]
            if len(blended_level.shape) == 3:
                w = np.expand_dims(w, axis=2)
            blended_level += w * laplacian_pyramids[i][level]
        blended_pyramid.append(blended_level)

    # Reconstruct from blended pyramid
    result = _reconstruct_from_laplacian(blended_pyramid)
    result = np.clip(result, 0, 255).astype(np.uint8)

    return result


# ============================================
# ROUTES
# ============================================

@app.get("/")
async def root():
    """API info and capabilities."""
    return {
        "name": "AutoHDR Clone API",
        "version": "1.0.0",
        "status": "running",
        "capabilities": {
            "raw_support": HAS_RAWPY,
            "processor": HAS_PROCESSOR,
            "opencv": CV2_VERSION,
        },
        "supported_formats": {
            "standard": ["jpg", "jpeg", "png", "tiff", "bmp", "webp"],
            "raw": list(RAW_EXTENSIONS) if HAS_RAWPY else []
        },
        "endpoints": {
            "POST /process": "Main processing endpoint",
            "GET /health": "Health check",
            "GET /test": "Test processing pipeline",
            "GET /docs": "API documentation"
        }
    }


@app.get("/health")
async def health():
    """Health check - returns system status."""
    return {
        "status": "healthy",
        "components": {
            "rawpy": {"installed": HAS_RAWPY, "version": RAWPY_VERSION},
            "opencv": {"installed": True, "version": CV2_VERSION},
            "processor": {"installed": HAS_PROCESSOR},
            "ai_processor": {"installed": HAS_AI_PROCESSOR, "note": "SAM + YOLOv8 + LaMa"},
        },
        "raw_formats_supported": len(RAW_EXTENSIONS) if HAS_RAWPY else 0,
        "quality_level": "90% AutoHDR" if HAS_AI_PROCESSOR else "60% AutoHDR (install ai-requirements.txt for 90%)",
    }


@app.get("/test")
async def test_processing():
    """
    Test the processing pipeline with a generated gradient image.
    Returns a processed test image to verify the pipeline works.
    """
    if not HAS_PROCESSOR:
        raise HTTPException(500, "Processor not available")

    try:
        start_time = time.time()

        # Create a test gradient image (simulates a photo with shadows and highlights)
        height, width = 480, 640
        test_image = np.zeros((height, width, 3), dtype=np.uint8)

        # Gradient background (sky simulation)
        for y in range(height):
            ratio = y / height
            test_image[y, :] = [
                int(255 * (1 - ratio * 0.5)),  # Blue (sky to ground)
                int(200 * (1 - ratio * 0.3)),  # Green
                int(180 * (1 - ratio * 0.4)),  # Red
            ]

        # Add some "windows" (bright rectangles)
        cv2.rectangle(test_image, (100, 200), (200, 350), (255, 255, 255), -1)
        cv2.rectangle(test_image, (450, 180), (550, 320), (255, 255, 255), -1)

        # Add "shadows" (dark areas)
        cv2.rectangle(test_image, (250, 350), (400, 450), (30, 30, 30), -1)

        # Process with default settings
        processor = AutoHDRProcessor(ProcessingSettings())
        result = processor.process(test_image)

        elapsed_ms = (time.time() - start_time) * 1000

        # Encode as JPEG
        _, buffer = cv2.imencode('.jpg', result, [cv2.IMWRITE_JPEG_QUALITY, 85])

        return Response(
            content=buffer.tobytes(),
            media_type="image/jpeg",
            headers={
                "Content-Disposition": "inline; filename=test_result.jpg",
                "X-Processing-Time-Ms": str(round(elapsed_ms, 2)),
            }
        )

    except Exception as e:
        raise HTTPException(500, f"Test failed: {str(e)}")


# ============================================
# MAIN PROCESSING ENDPOINT (matches Vercel frontend)
# ============================================

@app.post("/process")
async def process_images(
    images: List[UploadFile] = File(..., description="Images to process"),
    mode: str = Query("hdr", description="Processing mode: hdr or twilight"),
    brightness: float = Query(0, ge=-2, le=2),
    contrast: float = Query(0, ge=-2, le=2),
    vibrance: float = Query(0, ge=-2, le=2),
    whiteBalance: float = Query(0, ge=-2, le=2),
    ai: bool = Query(False, description="Use AI-enhanced processing (SAM, YOLOv8, LaMa)"),
    grass: bool = Query(False, description="Enhance grass (vibrant green)"),
    signs: bool = Query(False, description="Remove signs (requires ai=true for best results)"),
):
    """
    Main processing endpoint - matches Vercel frontend API.

    - **HDR mode**: Upload 2-9 bracketed exposures, merges with Mertens fusion
    - **Twilight mode**: Upload 1 daytime photo, converts to dusk
    - **AI mode**: Use SAM + YOLOv8 + LaMa for 90% AutoHDR quality

    Supports all RAW formats: ARW, CR2, NEF, DNG, etc.
    """
    start_time = time.time()

    # Validate
    if not images:
        raise HTTPException(400, "No images provided")

    if not HAS_PROCESSOR:
        raise HTTPException(500, "Processor not available - check server logs")

    # Log request
    filenames = [img.filename for img in images]
    print(f"üì∏ Processing {len(images)} images: {filenames}")
    print(f"   Mode: {mode}, Settings: b={brightness}, c={contrast}, v={vibrance}, wb={whiteBalance}")

    try:
        # ==========================================
        # STEP 1: Read all images (including RAW)
        # ==========================================
        image_arrays = []
        for i, upload in enumerate(images):
            print(f"   Reading [{i+1}/{len(images)}]: {upload.filename}")
            try:
                img = await read_image_async(upload)
                print(f"   ‚úì Loaded: {img.shape[1]}x{img.shape[0]} pixels")
                image_arrays.append(img)
            except HTTPException:
                raise
            except Exception as e:
                raise HTTPException(400, f"Failed to read {upload.filename}: {str(e)}")

        # ==========================================
        # STEP 2: Merge brackets (if multiple images)
        # ==========================================
        if len(image_arrays) > 1:
            print(f"   Merging {len(image_arrays)} brackets with Laplacian pyramid fusion...")
            base_image = merge_brackets_laplacian(image_arrays)
        else:
            base_image = image_arrays[0]

        # ==========================================
        # STEP 3: Process (Basic or AI-Enhanced)
        # ==========================================
        if ai and HAS_AI_PROCESSOR:
            print("   ü§ñ Using AI-enhanced processing (SAM + YOLOv8 + LaMa)...")
            ai_settings = AIProcessingSettings(
                brightness=brightness,
                contrast=contrast,
                vibrance=vibrance,
                white_balance=whiteBalance,
                twilight_style="pink" if mode == "twilight" else None,
                grass_enhancement=grass,
                sign_removal=signs,
                use_ai_segmentation=True,
                use_ai_detection=True,
                use_ai_inpainting=signs,  # Only use LaMa if removing signs
            )
            processor = AIEnhancedProcessor(ai_settings)
            result = processor.process(base_image)
        else:
            if ai and not HAS_AI_PROCESSOR:
                print("   ‚ö†Ô∏è AI processor not available, using basic processing")
            print("   Applying HDR processing...")
            settings = ProcessingSettings(
                brightness=brightness,
                contrast=contrast,
                vibrance=vibrance,
                white_balance=whiteBalance,
                twilight_style="pink" if mode == "twilight" else None,
                grass_replacement=grass,
                sign_removal=signs,
            )
            processor = AutoHDRProcessor(settings)
            result = processor.process(base_image)

        # ==========================================
        # STEP 3: Encode and return
        # ==========================================
        result_bytes = image_to_bytes(result, ".jpg", quality=98)  # Full resolution output

        elapsed_ms = (time.time() - start_time) * 1000
        print(f"   ‚úì Complete in {elapsed_ms:.0f}ms, output: {len(result_bytes)} bytes")

        return Response(
            content=result_bytes,
            media_type="image/jpeg",
            headers={
                "Content-Disposition": f'attachment; filename="autohdr_{mode}.jpg"',
                "X-Processing-Time-Ms": str(round(elapsed_ms, 2)),
                "X-Images-Processed": str(len(images)),
            }
        )

    except HTTPException:
        raise
    except Exception as e:
        print(f"   ‚úó ERROR: {str(e)}")
        traceback.print_exc()
        raise HTTPException(500, f"Processing failed: {str(e)}")


# ============================================
# LEGACY ENDPOINTS (for direct API use)
# ============================================

@app.post("/hdr/merge")
async def hdr_merge(
    images: List[UploadFile] = File(...),
    method: str = Form("mertens"),
    tone_map: str = Form("reinhard"),
    align: bool = Form(True)
):
    """
    Merge multiple exposure brackets into HDR.

    Upload 2-9 images taken at different exposures.
    Returns the merged, tone-mapped result.
    """
    if len(images) < 2:
        raise HTTPException(400, "Need at least 2 images for HDR merge")
    if len(images) > 9:
        raise HTTPException(400, "Maximum 9 images supported")

    # Read all images (with RAW support)
    loaded_images = []
    for img_file in images:
        img = await read_image_async(img_file)
        loaded_images.append(img)

    # Use Mertens fusion (built-in, always works)
    result = merge_brackets_mertens(loaded_images)

    # Apply HDR processing
    processor = AutoHDRProcessor(ProcessingSettings())
    result = processor.process(result)

    # Return as JPEG
    result_bytes = image_to_bytes(result, ".jpg", quality=90)

    return StreamingResponse(
        io.BytesIO(result_bytes),
        media_type="image/jpeg",
        headers={"Content-Disposition": "attachment; filename=hdr_result.jpg"}
    )


@app.post("/effects/day-to-dusk")
async def day_to_dusk(
    image: UploadFile = File(...),
    style: str = Form("pink"),  # pink, blue, orange
    brightness: float = Form(-0.5),  # -2 to 2
):
    """
    Convert daytime exterior photo to twilight/dusk.

    Parameters:
    - style: Twilight color (pink, blue, orange)
    - brightness: Adjustment (-2 to 2, negative = darker)
    """
    # Read image (with RAW support)
    img = await read_image_async(image)

    # Configure for twilight
    settings = ProcessingSettings(
        brightness=brightness,
        twilight_style=style if style in ["pink", "blue", "orange"] else "pink"
    )

    # Process
    processor = AutoHDRProcessor(settings)
    result = processor.process(img)

    # Return as JPEG
    result_bytes = image_to_bytes(result, ".jpg")

    return StreamingResponse(
        io.BytesIO(result_bytes),
        media_type="image/jpeg",
        headers={"Content-Disposition": "attachment; filename=twilight_result.jpg"}
    )


@app.post("/edit/remove")
async def remove_object(
    image: UploadFile = File(...),
    prompt: str = Form(None),
    mask: UploadFile = File(None)
):
    """
    Remove objects from image using AI inpainting.

    Provide either:
    - prompt: Text description of what to remove (e.g., "remove the car")
    - mask: Binary mask image where white = area to remove

    Coming soon: Requires LaMa or IOPaint integration.
    """
    # TODO: Implement with LaMa / Grounded-SAM
    raise HTTPException(501, "Object removal coming soon. Use IOPaint for now: https://github.com/Sanster/IOPaint")


@app.post("/edit/sky-replace")
async def sky_replace(
    image: UploadFile = File(...),
    sky_type: str = Form("twilight")  # twilight, blue, dramatic, cloudy
):
    """
    Replace sky in image.

    Coming soon: Requires sky segmentation model.
    """
    # TODO: Implement with sky segmentation
    raise HTTPException(501, "Sky replacement coming soon. Requires segmentation model.")


# ============================================
# MAIN
# ============================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
